# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

AI TOOLS REQUIRED:

ChatGPT (OpenAI GPT Model)

Optional alternatives for comparison: Gemini, Claude, Copilot

Internet connection for online execution

Text editor for report documentation

EXPLANATION:
1. Define the Two Prompt Types

Naïve Prompt:
A simple, vague, or general instruction with little detail. It relies on the AI’s assumptions and may lead to less accurate or incomplete answers.

Basic Prompt (Refined Prompt):
A clear, specific, and well-structured instruction that provides context, intent, and output expectations — resulting in a more targeted and high-quality response.

2. Prepare Multiple Test Scenarios

Four diverse scenarios were selected to test the difference between naïve and basic prompts:

Creative Story Generation

Factual Question Answering

Concept/Article Summarization

Providing Advice or Recommendations

3. Run Experiments with ChatGPT
Scenario 1: Creative Story Generation
| Prompt Type      | Example Prompt                                                                                                    | Sample Output                                                     | Observations                                                             |
| ---------------- | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------ |
| **Naïve Prompt** | “Write a story about a robot.”                                                                                    | A short story about a generic robot with limited detail.          | Output lacked emotional depth, character development, and setting.       |
| **Basic Prompt** | “Write a 200-word story about a robot who learns to experience human emotions while living in a futuristic city.” | Detailed, coherent, emotionally rich story with character growth. | Clear improvement in creativity and coherence due to structured context. |

Scenario 2: Factual Question Answering
| Prompt Type      | Example Prompt                                                                                                       | Sample Output                                                         | Observations                                            |
| ---------------- | -------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------- |
| **Naïve Prompt** | “Tell me about AI.”                                                                                                  | A vague definition of AI without structure.                           | Output was too broad and missed key applications.       |
| **Basic Prompt** | “Explain Artificial Intelligence, its main types (Narrow, General, Super AI), and give two real-world applications.” | Organized answer with examples like Chatbots and Autonomous Vehicles. | More accurate and structured because of clear guidance. |

Scenario 3: Concept Summarization

| Prompt Type      | Example Prompt                                                                                                      | Sample Output                                                      | Observations                                                     |
| ---------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ | ---------------------------------------------------------------- |
| **Naïve Prompt** | “Summarize blockchain.”                                                                                             | One paragraph with basic definition.                               | Lacked detail on how it works.                                   |
| **Basic Prompt** | “Summarize the concept of blockchain in 100 words, explaining how blocks, transactions, and decentralization work.” | Concise, structured summary including key components and examples. | Higher factual accuracy and clarity due to specific constraints. |

Scenario 4: Providing Advice or Recommendation

| Prompt Type      | Example Prompt                                                                                                                     | Sample Output                                                            | Observations                                                            |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------------------------- |
| **Naïve Prompt** | “How to study better?”                                                                                                             | General advice like “read books” or “focus more.”                        | Too generic, lacking personalization.                                   |
| **Basic Prompt** | “Give 5 practical study techniques for engineering students preparing for final exams, focusing on time management and retention.” | Detailed, well-organized, and practical advice tailored to the audience. | Improved depth and usability due to specific goal and audience context. |

4. Evaluation of Responses
Comparison Table

| Scenario         | Naïve Prompt Quality | Basic Prompt Quality | Accuracy (1–5) | Depth (1–5) | Improvement Observed    |
| ---------------- | -------------------- | -------------------- | -------------- | ----------- | ----------------------- |
| Story Generation | Moderate             | Excellent            | 3 → 5          | 3 → 5       | Strong improvement      |
| Factual Answer   | Fair                 | Excellent            | 2 → 5          | 3 → 5       | Major improvement       |
| Summarization    | Good                 | Excellent            | 3 → 5          | 3 → 4       | Moderate improvement    |
| Advice           | Fair                 | Very Good            | 2 → 5          | 2 → 5       | Significant improvement |

5. Analysis

Quality: Basic prompts consistently generated more coherent, informative, and contextually relevant responses.

Accuracy: Structured prompts helped the model avoid generalizations and stay on-topic.

Depth: Clear instructions allowed ChatGPT to explore ideas in greater detail and relevance.

Exceptions: In very simple tasks, naïve prompts were adequate (e.g., generic summaries).

Overall, prompt clarity has a direct correlation with the model’s output accuracy and usefulness.

6. Summary of Findings

Naïve prompts lead to generic or incomplete answers due to lack of context.

Basic (structured) prompts yield precise, high-quality outputs suitable for academic or professional use.

Optimal Prompting Strategy: Always include context, goal, and constraints in the prompt.

ChatGPT performs best when given a clear, well-structured prompt that defines purpose, tone, and scope.

OUTPUT

The experiments successfully demonstrated the difference between naïve and basic prompting patterns through multiple scenarios. The results showed that structured prompts significantly enhance the quality, accuracy, and depth of ChatGPT’s responses.

RESULT

Thus, the prompt for the above said problem was executed successfully, and it was concluded that refined, structured prompts consistently produce superior results compared to unstructured, naïve ones across all tested scenarios.
